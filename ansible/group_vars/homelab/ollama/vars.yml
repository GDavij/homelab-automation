---
# ==============================================================================
# Ollama Role - Variables
# ==============================================================================
# Local LLM server (Llama 3, Mistral, CodeLlama, etc.)
# Port: 11434 (API)
# GPU: NVIDIA RTX 3060 12GB
# See: docs/ENVIRONMENT_VARIABLES.md#ollama-llm-server
# ==============================================================================

# ------------------------------------------------------------------------------
# Version Configuration
# ------------------------------------------------------------------------------
OLLAMA_VERSION: "latest"  # Docker image version

# ------------------------------------------------------------------------------
# Storage Configuration
# ------------------------------------------------------------------------------
OLLAMA_MODELS_PATH: "{{ COLD_STORAGE_PATH }}/ai/ollama/models"  # Downloaded models (can be large)

# ------------------------------------------------------------------------------
# Network Configuration
# ------------------------------------------------------------------------------
OLLAMA_SUBDOMAIN: "ollama"  # For NGINX proxy: ollama.yourdomain.local


# ==============================================================================
# Service Information
# ==============================================================================
# - API Endpoint: http://{{ TAILSCALE_IP_ADDRESS }}:11434
# - Pull models: docker exec -it ollama ollama pull llama3
# - List models: docker exec -it ollama ollama list
# - Model storage: Can be 4-7GB per model
# ==============================================================================
